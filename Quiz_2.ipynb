{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 2 Starter Code\n",
    "\n",
    "You can use this notebook to arrive at the answers for the quiz on Model Evaluation. It also provides you some hints and starter code.\n",
    "\n",
    "For this quiz, we use a dataset derived from a variety of sources, including the Chicago Open Data portal and the Census. For details, see the quiz.\n",
    "\n",
    "First, we'll load the data for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "crime_acs = pd.read_csv(\"../data/crime_acs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your target variable will be Arrest. Your classifier should consider the following features:\n",
    "\n",
    "- `Primary Type`\n",
    "- `Ward`\n",
    "- `FBI Code`\n",
    "- `Percent White`\n",
    "- `Percent Black`\n",
    "- `Median Income`\n",
    "\n",
    "## Part 1: Data Preparation\n",
    "\n",
    "\n",
    "### Question 1\n",
    "First, create the training and testing sets. Use a split of .2, and a `random_state` of 0\n",
    "As a quick sanity check, how many rows are in the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                        Date        Primary Type  \\\n",
       "35016   2019-02-03 04:57:00     CRIMINAL DAMAGE   \n",
       "56096   2019-02-21 02:21:00               THEFT   \n",
       "78820   2019-06-24 11:20:00             BATTERY   \n",
       "139403  2019-07-19 03:05:00             BATTERY   \n",
       "90866   2019-08-16 01:38:00           NARCOTICS   \n",
       "...                     ...                 ...   \n",
       "176963  2019-07-25 09:00:00  DECEPTIVE PRACTICE   \n",
       "117952  2019-09-13 01:05:00             BATTERY   \n",
       "173685  2019-05-09 10:44:00        PROSTITUTION   \n",
       "43567   2019-09-05 08:19:00     CRIMINAL DAMAGE   \n",
       "199340  2019-09-03 11:21:00             BATTERY   \n",
       "\n",
       "                                            Description  \\\n",
       "35016                                       TO PROPERTY   \n",
       "56096                                      RETAIL THEFT   \n",
       "78820                                            SIMPLE   \n",
       "139403                          DOMESTIC BATTERY SIMPLE   \n",
       "90866                               POSS: HEROIN(WHITE)   \n",
       "...                                                 ...   \n",
       "176963              FINANCIAL IDENTITY THEFT OVER $ 300   \n",
       "117952                          DOMESTIC BATTERY SIMPLE   \n",
       "173685                             SOLICIT FOR BUSINESS   \n",
       "43567                                       TO PROPERTY   \n",
       "199340  AGGRAVATED DOMESTIC BATTERY: KNIFE/CUTTING INST   \n",
       "\n",
       "           Location Description  Arrest  Domestic  Beat  District  Ward  \\\n",
       "35016                 APARTMENT   False     False   311         3  20.0   \n",
       "56096          DEPARTMENT STORE    True     False  1732        17  35.0   \n",
       "78820                 RESIDENCE   False      True   522         5  34.0   \n",
       "139403                   STREET   False      True  1533        15  28.0   \n",
       "90866   CHA PARKING LOT/GROUNDS    True     False  1222        12  27.0   \n",
       "...                         ...     ...       ...   ...       ...   ...   \n",
       "176963                    OTHER   False     False  1824        18   2.0   \n",
       "117952                APARTMENT   False      True   823         8  16.0   \n",
       "173685                   STREET    True     False  1131        11  24.0   \n",
       "43567                 RESIDENCE   False     False  2535        25  26.0   \n",
       "199340                APARTMENT    True      True  1112        11  27.0   \n",
       "\n",
       "       FBI Code  Percent White  Percent Black  Median Income  \\\n",
       "35016        14       0.000000       0.861458            NaN   \n",
       "56096        06       0.720412       0.000000        62212.0   \n",
       "78820       08B       0.000000       1.000000        21792.0   \n",
       "139403      08B       0.000000       0.989492        12011.0   \n",
       "90866        18       0.092360       0.835804        45893.0   \n",
       "...         ...            ...            ...            ...   \n",
       "176963       11       0.919417       0.062136        81346.0   \n",
       "117952      08B       0.146138       0.205985        42722.0   \n",
       "173685       16       0.003839       0.966731        20703.0   \n",
       "43567        14       0.807598       0.090686        45000.0   \n",
       "199340      04B       0.000000       0.938835        36667.0   \n",
       "\n",
       "                          Block  Community Area  \n",
       "35016       061XX S INDIANA AVE            40.0  \n",
       "56096       035XX N KIMBALL AVE            21.0  \n",
       "78820         118XX S PERRY AVE            53.0  \n",
       "139403        050XX W MONROE ST            25.0  \n",
       "90866        030XX W MADISON ST            27.0  \n",
       "...                         ...             ...  \n",
       "176963       014XX N STATE PKWY             8.0  \n",
       "117952       061XX S ALBANY AVE            66.0  \n",
       "173685        045XX W FIFTH AVE            26.0  \n",
       "43567   019XX N SPRINGFIELD AVE            20.0  \n",
       "199340     007XX N RIDGEWAY AVE            23.0  \n",
       "\n",
       "[205805 rows x 15 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split into training and testing sets using random_state=0. Then, find the number of rows in the training set.\n",
    "c_train, c_test= train_test_split(crime_acs, test_size=0.2, random_state=0)\n",
    "c_train.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Next, convert the label \"Arrest\" into a numerical (rather than boolean) feature in both your training and testing data (i.e., a 1 for an arrest, and a 0 for no arrest).\n",
    "\n",
    "In the training data, what percentage of recorded crimes resulted in an arrest? (express as a decimal between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21383834211996794\n"
     ]
    }
   ],
   "source": [
    "# For the Arrest column, convert True to 1 and False to 0\n",
    "# Do this for both the train and test sets\n",
    "# Convertir la columna 'Arrest' en valores numéricos (1/0)\n",
    "c_train['Arrest'] = c_train['Arrest'].astype(int)\n",
    "c_test['Arrest'] = c_test['Arrest'].astype(int)\n",
    "\n",
    "# Now, in the training data, find the fraction of crimes that resulted in arrest\n",
    "# Your answer may look like train_df['Arrest'].value_counts() / train_df.shape[0]\n",
    "p_arrest_c_train = c_train['Arrest'].value_counts() / c_train.shape[0]\n",
    "\n",
    "print(p_arrest_c_train[1])\n",
    "\n",
    "# Hint: your answer should be a number between .2 and .3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Next, we will want to pre-process the continuous numeric features (Percent White, Percent Black, Median Income). This will mean normalizing each feature, and imputing missing values.\n",
    "\n",
    "First, note that administrative data often uses encodings to indicate missing data.\n",
    "\n",
    "So we should make sure to perform sanity checks (e.g. ensure that your percentages fall between 0 and 1, that income follows a reasonable distribution, etc.) \n",
    "\n",
    "The `Median Income` uses such an administrative code for some missing values--what is that code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    nan  62212.  21792. ...  37670. 104444. 100788.]\n",
      "float64\n",
      "-666666666.0\n"
     ]
    }
   ],
   "source": [
    "# Take a  look at the Median Income column--some values may be NaN, but some will be an administrative code\n",
    "\n",
    "print(c_train['Median Income'].unique())\n",
    "print(c_train['Median Income'].dtype)\n",
    "unique_values = set(c_train[\"Median Income\"].unique())\n",
    "for i in unique_values:\n",
    "    if i == -42:\n",
    "        print(i)\n",
    "    elif i == -999999:\n",
    "        print(i)\n",
    "    elif i == -666666666.0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Replace the administrative code in question 3 with NaN for the training and testing sets. Then, normalize `Percent White`, `Percent Black`, and `Median Income` in the way that we have learned:\n",
    "1. find the mean and standard deviation in the training set.\n",
    "\n",
    "2. Subtract the training mean from each value, then divide by the training standard deviation.\n",
    "\n",
    "Finally, replace the missing values with the mean in its column.\n",
    "\n",
    "After going through these steps--normalizing and imputing missing values--what is the mean value in the test set for \"Median Income\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 1. First, replace the administrative code you found previously with NaN for the test and train sets.\n",
    "admin_code = -666666666.0\n",
    "c_train[\"Median Income\"].replace(admin_code, np.NaN, inplace=True)\n",
    "c_test[\"Median Income\"].replace(admin_code, np.NaN, inplace=True)\n",
    "\n",
    "print(c_train['Median Income'].unique())\n",
    "print(c_train['Median Income'].dtype)\n",
    "unique_values = set(c_train[\"Median Income\"].unique())\n",
    "\n",
    "# Comprobar si se ha eliminado el administrative code\n",
    "for i in unique_values:\n",
    "    if i == -42:\n",
    "        print(i)\n",
    "    elif i == -999999:\n",
    "        print(i)\n",
    "    elif i == -666666666.0:\n",
    "        print(i)\n",
    "# 2. Now, normalize. You can do this a few different ways, but you might consider writing a for loop.\n",
    "# If you go that path the code below gets you started:\n",
    "cols = [\"Percent White\", \"Percent Black\", \"Median Income\"]\n",
    "for col in cols:\n",
    "    # Calcular media y desviación estándar del conjunto de entrenamiento\n",
    "    train_mean = c_train[col].mean()\n",
    "    train_std = c_train[col].std()\n",
    "\n",
    "    # Normalizar el conjunto de entrenamiento\n",
    "    c_train[col] = (c_train[col] - train_mean) / train_std\n",
    "\n",
    "    # Normalizar el conjunto de prueba (usando la media y desviación del conjunto de entrenamiento)\n",
    "    c_test[col] = (c_test[col] - train_mean) / train_std\n",
    "\n",
    "    # Reemplazar valores faltantes por la media del conjunto de entrenamiento\n",
    "    c_train[col].replace(np.NaN, train_mean, inplace=True)\n",
    "    c_test[col].replace(np.NaN, train_mean, inplace=True)\n",
    "\n",
    "# Now, the sanity check: after this process, get the test mean for Median Income\n",
    "MI_train_mean = c_train[\"Median Income\"].mean()\n",
    "print(MI_train_mean)\n",
    "\n",
    "# Hint: It should be between 2000 and 3000\n",
    "#Sol del test: 2044"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question  5\n",
    "\n",
    "There is just one more data preparation step, encoding features from the categorical variables (\"Primary Type\", \"Ward\", and \"FBI Code\"). The standard way to encode categorical features in machine learning is through one-hot encoding. The function \"pd.get_dummies\" will be useful. \n",
    "\n",
    "An inherent issue arises with this approach when a value appears in either your training or testing data, but not in both. If a value appears in your training set but not your testing set, create a column with all 0's in your testing set. If a value appears in your testing set but not your training set, drop it from your testing data. \n",
    "So:\n",
    "1.  Use get_dummies to one-hot encode \"Primary Type\", \"Ward\", and \"FBI Code\"\n",
    "2. For features that appear in the training data but not testing data, create them and populate them with 0's\n",
    "3. Drop features that appear in the testing data but not training data\n",
    "4. Finally, make sure that the columns that we aren't going to use for classification are dropped. These are: [`Date`, `Description`, `Location Description`,`Domestic`, `Beat`, `District`, `Block`, `Community Area`]\n",
    "\n",
    "How many columns are now in your training and testing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in training data: 112\n",
      "Number of columns in testing data: 112\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 1. One-hot encode these columns using get_dummies()\n",
    "encode_cols = ['Primary Type', 'Ward', 'FBI Code']\n",
    "\n",
    "#Your code to on-hot encode here\n",
    "train_encoded = pd.get_dummies(c_train, columns=encode_cols)\n",
    "test_encoded = pd.get_dummies(c_test, columns=encode_cols)\n",
    "\n",
    "# Now, here is some code to get the columns as lists:\n",
    "train_cols = set(train_encoded.columns.to_list())\n",
    "test_cols = set(test_encoded.columns.to_list())\n",
    "\n",
    "# 2. For features in the training data but not testing data, create them and populate with 0\n",
    "# Your code to find the columns in training but not testing\n",
    "# Sample code to set them to zero:\n",
    "in_training_not_testing = train_cols - test_cols\n",
    "for missing_col in in_training_not_testing:\n",
    "    test_encoded[missing_col] = 0\n",
    "    \n",
    "# 3. For features in testing but not training, drop them\n",
    "# Your code to find features in testing but not training\n",
    "# Sample code to drop them\n",
    "in_testing_not_training = test_cols - train_cols\n",
    "for extra_col in in_testing_not_training:\n",
    "    test_encoded.drop(columns=extra_col, inplace=True)\n",
    "\n",
    "# Finally, drop the columns that will not be used for your model from both--these are given above\n",
    "# Eliminar las columnas que no serán usadas para el modelo\n",
    "columns_to_drop = [\"Date\", \"Description\", \"Location Description\", \"Domestic\", \"Beat\", \"District\", \"Block\", \"Community Area\"]\n",
    "                               \n",
    "train_encoded.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "test_encoded.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "                                  \n",
    "# How many are you left with?\n",
    "print(f\"Number of columns in training data: {train_encoded.shape[1]}\")\n",
    "print(f\"Number of columns in testing data: {test_encoded.shape[1]}\")          \n",
    "# Hint: It should be between 100 and 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below performs a sanity check by verifying that your training set and your testing set wound up with the same features, and that you have successfully imputed all missing values.\n",
    "\n",
    "You can run it before moving on to modeling as a simple check to make sure you're in a good place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(train_df, test_df): \n",
    "    \n",
    "    # Sort features alphabetically\n",
    "    train_df = train_df.reindex(sorted(train_df.columns), axis=1)\n",
    "    test_df = test_df.reindex(sorted(test_df.columns), axis=1)\n",
    "\n",
    "    # Check that they have the same features\n",
    "    if (train_df.columns == test_df.columns).all():\n",
    "        print(\"Success: Features match\")\n",
    "\n",
    "    # Check that no NAs remain\n",
    "    if  not train_df.isna().sum().astype(bool).any() and \\\n",
    "        not test_df.isna().sum().astype(bool).any():\n",
    "        print(\"Success: No NAs remain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Modeling\n",
    "\n",
    "In this part, you will train and evaluate models using different classification techniques and hyperparameters. You will now need to separate your train and test data into the training and testing features and target variables.\n",
    "\n",
    "### Question 6\n",
    "\n",
    "As mentioned, the target variable will be `Arrest`; train on the other features.\n",
    "\n",
    "First, logistic regression. Use a GridSearch with 2-fold cross validation, and try tuning the penalty and the value for C. \n",
    "For the penalty, try l2 and no penalty.\n",
    "For C, try the values (0.01, 0.1, 1).\n",
    "Evaluate based on accuracy.\n",
    "Which combination of penalty and C gives the best results from the GridSearch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need it, you can load train and test data that has been prepared for you, per the steps above\n",
    "train_df = pd.read_csv(\"../data/quiz2_train.csv\")\n",
    "test_df = pd.read_csv(\"../data/quiz2_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([16.40823913,  0.08118677, 29.06654406,  0.08343661, 28.95643723,\n",
      "        0.09478784]), 'std_fit_time': array([0.31228924, 0.00171804, 0.12913537, 0.0024153 , 0.02910602,\n",
      "       0.00710058]), 'mean_score_time': array([0.29956031, 0.        , 0.30224431, 0.        , 0.30979848,\n",
      "       0.        ]), 'std_score_time': array([0.0006597 , 0.        , 0.00142753, 0.        , 0.00279164,\n",
      "       0.        ]), 'param_C': masked_array(data=[0.01, 0.01, 0.1, 0.1, 1, 1],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_penalty': masked_array(data=['l2', None, 'l2', None, 'l2', None],\n",
      "             mask=[False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.01, 'penalty': 'l2'}, {'C': 0.01, 'penalty': None}, {'C': 0.1, 'penalty': 'l2'}, {'C': 0.1, 'penalty': None}, {'C': 1, 'penalty': 'l2'}, {'C': 1, 'penalty': None}], 'split0_test_score': array([0.86520315,        nan, 0.86634986,        nan, 0.86634015,\n",
      "              nan]), 'split1_test_score': array([0.86592097,        nan, 0.8670677 ,        nan, 0.86665954,\n",
      "              nan]), 'mean_test_score': array([0.86556206,        nan, 0.86670878,        nan, 0.86649984,\n",
      "              nan]), 'std_test_score': array([0.00035891,        nan, 0.00035892,        nan, 0.0001597 ,\n",
      "              nan]), 'rank_test_score': array([3, 4, 1, 5, 2, 6], dtype=int32)}\n",
      "Mejor combinación de hiperparámetros: {'C': 0.1, 'penalty': 'l2'}\n",
      "Mejor puntuación media: 0.8667087794704502\n",
      "Precisión en el conjunto de prueba: 0.826692839928477\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Separar las características (X) y la variable objetivo (y)\n",
    "X_train = train_df.drop(columns=['Arrest'])  # Eliminar la columna 'Arrest' para las características\n",
    "y_train = train_df['Arrest']  # La variable objetivo es 'Arrest'\n",
    "\n",
    "X_test = test_df.drop(columns=['Arrest'])  # Similar para los datos de prueba\n",
    "y_test = test_df['Arrest']\n",
    "\n",
    "# 2. Configuración de los parámetros a probar en el GridSearch\n",
    "param_grid = {\n",
    "    'penalty': ['l2', None],  # Probar 'l2' y sin penalización\n",
    "    'C': [0.01, 0.1, 1],  # Probar diferentes valores de C\n",
    "}\n",
    "\n",
    "# 3. Crear el modelo de regresión logística\n",
    "log_reg = LogisticRegression(random_state=0)\n",
    "\n",
    "# 4. Configurar el GridSearchCV con 2 pliegues de validación cruzada\n",
    "grid_model = GridSearchCV(log_reg, param_grid, cv=2, scoring='accuracy', verbose=1)\n",
    "\n",
    "# 5. Entrenar el modelo con los datos de entrenamiento\n",
    "grid_model.fit(X_train, y_train)\n",
    "\n",
    "# 6. Ver los resultados del GridSearch\n",
    "#print(\"Mejor combinación de hiperparámetros:\", grid_model.best_params_)\n",
    "#print(\"Mejor puntuación de precisión en la validación cruzada:\", grid_model.best_score_)\n",
    "\n",
    "\n",
    "# Mostrar los resultados del GridSearch\n",
    "print(grid_model.cv_results_)\n",
    "\n",
    "# Mostrar la mejor combinación de hiperparámetros\n",
    "print(f\"Mejor combinación de hiperparámetros: {grid_model.best_params_}\")\n",
    "\n",
    "# Si quieres ver la media de las puntuaciones de cada combinación probada\n",
    "print(f\"Mejor puntuación media: {grid_model.best_score_}\")\n",
    "\n",
    "# 7. Evaluar el modelo en el conjunto de prueba\n",
    "y_pred = grid_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Precisión en el conjunto de prueba: {test_accuracy}\")\n",
    "\n",
    "\n",
    "# Try l2 and no penalty\n",
    "# For C, try .01, .1, 1\n",
    "# Use 2-fold cross validation\n",
    "# Use random_state = 0--e.g., LogisticRegression(random_state=0)\n",
    "# What combination gives the best mean accuracy?\n",
    "# Hint: you can access results using grid_model.cv_results_\n",
    "# Según el test Sin penalty y c= 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Next, try tuning a linear support vector machine classifier. Again, use 2-fold cross-validation, and for C try the values (0.01, 0.1, 1). Once again, score on accuracy. Which value for C produces the best score in the GridSearch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] C=0.01 ..........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............................. C=0.01, score=0.865, total=18.5min\n",
      "[CV] C=0.01 ..........................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 18.5min remaining:    0.0s\n"
     ]
    }
   ],
   "source": [
    "# Same as above, but this time with a linear SVM.\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Separar las características (X) y la variable objetivo (y)\n",
    "X_train = train_df.drop(columns=['Arrest'])  # Eliminar la columna 'Arrest' para las características\n",
    "y_train = train_df['Arrest']  # La variable objetivo es 'Arrest'\n",
    "\n",
    "X_test = test_df.drop(columns=['Arrest'])  # Similar para los datos de prueba\n",
    "y_test = test_df['Arrest']\n",
    "# Definir el clasificador SVM con kernel lineal\n",
    "svm_model = SVC(kernel='linear', random_state=0)\n",
    "\n",
    "# Definir los valores de C para probar\n",
    "param_grid = {'C': [0.01, 0.1, 1]}\n",
    "\n",
    "# Configurar GridSearchCV con validación cruzada de 2 pliegues\n",
    "grid_model = GridSearchCV(svm_model, param_grid, cv=2, scoring='accuracy', verbose=3)\n",
    "\n",
    "# Ajustar el modelo\n",
    "grid_model.fit(X_train, y_train)  # Asegúrate de haber separado previamente las características y la variable objetivo\n",
    "\n",
    "# Ver los mejores parámetros\n",
    "print(f\"Best C value: {grid_model.best_params_['C']}\")\n",
    "\n",
    "# También puedes acceder a los resultados completos\n",
    "print(grid_model.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "The last model type you want to consider is a Naive Bayes classifier. Likewise, use 2-fold cross-validation, and evaluate on accuracy. What is the mean accuracy score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.8623648578121235\n"
     ]
    }
   ],
   "source": [
    "# Finally, naive Bayes. No need to use GridSearch for this one, since you're not tuning any hyperparameters\n",
    "# You can use cross_val_score:\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# do 2-fold cross-validation and give the mean accuracy\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Separar características y la variable objetivo\n",
    "X_train = train_df.drop('Arrest', axis=1)  # Las características (sin la columna \"Arrest\")\n",
    "y_train = train_df['Arrest']  # La variable objetivo (\"Arrest\")\n",
    "\n",
    "# Crear el clasificador Naive Bayes\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Realizar la validación cruzada con 2 pliegues y calcular la precisión\n",
    "cv_scores = cross_val_score(nb_model, X_train, y_train, cv=2, scoring='accuracy')\n",
    "\n",
    "# Calcular la media de la precisión\n",
    "mean_accuracy = cv_scores.mean()\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(f'Mean accuracy: {mean_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Finally, we want to determine which features are most important.\n",
    "\n",
    "For these examples, the SVM with the parameters in question 7 will have slightly edged out the others. Train a Linear SVM classifier with that value for C. \n",
    "\n",
    "For the features used to train this model, which has the largest positive contribution towards classification as positive? (i.e., which has the largest positvie coefficient?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your Linear SVM with the best value you found for C\n",
    "# You can access model coefficients using model.coef_ (the array will be in the same order as the features)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Obtener el mejor valor de C encontrado en la pregunta anterior (supongamos que es 1, basado en los resultados de GridSearch)\n",
    "best_C = 0.01\n",
    "\n",
    "# Crear el modelo SVM\n",
    "svm_model = SVC(C=best_C, kernel='linear')\n",
    "\n",
    "# Entrenarlo con los datos de entrenamiento\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los coeficientes del modelo\n",
    "coefficients = svm_model.coef_\n",
    "\n",
    "# Convertir los coeficientes en un array y mostrar los resultados\n",
    "print(\"Coeficientes del modelo:\")\n",
    "print(coefficients)\n",
    "\n",
    "# Obtener cuál de las características tiene el mayor coeficiente positivo\n",
    "max_positive_idx = coefficients.argmax()\n",
    "features = X_train.columns  # Obtener nombres de las características\n",
    "\n",
    "# Mostrar la característica con la mayor contribución positiva\n",
    "print(f\"La característica con la mayor contribución positiva es: {features[max_positive_idx]}\")\n",
    "\n",
    "#Sol: Narcotica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
